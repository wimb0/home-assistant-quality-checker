# /// script
# requires-python = ">=3.13"
# dependencies = [
#     "google-genai==1.7.0",
#     "pyyaml",
#     "requests",
#     "tiktoken",
# ]
# ///
"""Check quality scale for an integration."""

import datetime
import logging
import json
import ast
import sys
import yaml
from pathlib import Path

from google import genai
import requests
import tiktoken

# Setting up logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

FREE_MODEL = "gemini-2.5-flash-preview-04-17"
PAID_MODEL = "gemini-2.5-pro-preview-05-06"
QUALITY_SCALE_RULE_RAW_URL = "https://raw.githubusercontent.com/home-assistant/developers.home-assistant/refs/heads/master/docs/core/integration-quality-scale/rules/{}.md"
QUALITY_SCALE_RULE_DOCS_URL = (
    "https://developers.home-assistant.io/docs/core/integration-quality-scale/rules/{}"
)
DOCS_URL = "https://www.home-assistant.io/integrations/{}/"

SCRIPT_DIR = Path(__file__).parent
OUTPUT_DIR = SCRIPT_DIR / "generated"

RULE_REVIEW_PROMPT = """
You are an expert Home Assistant code reviewer specializing in the Integration Quality Scale. Your task is to meticulously analyze a given Home Assistant integration's code against a specific quality scale rule.

**Goal:**
Generate a Markdown report assessing if the `{integration}` follows the rule `{rule}`.

**Input You Will Receive:**
1.  The name of the integration: {integration}.
2.  The name of the rule being checked: {rule}.
3.  The full text and requirements of the rule.
4.  The code for the integration

**Output Requirements:**

The report must be in Markdown format and determine one of three statuses:
*   **"todo"**: The rule applies to this integration, AND the integration currently does NOT follow it.
*   **"done"**: The rule applies to this integration, AND the integration fully follows it.
*   **"exempt"**: The rule does NOT apply to this integration.

**Report Structure:**

1.  **Title:**
    ```markdown
    # {integration}: {rule}
    ```

2.  **Information Table:**
    ```markdown
    | Info   | Value                                                                    |
    |--------|--------------------------------------------------------------------------|
    | Name   | [{integration}](https://www.home-assistant.io/integrations/{integration}/) |
    | Rule   | [{rule}]({rule_url})                                                     |
    | Status | **todo** OR **done** OR **exempt**                                       |
    | Reason | (Only include if Status is "exempt". Explain why the rule does not apply.) |
    ```

3.  **Overview Section:**
    *   **Heading:** `## Overview`
    *   **Content:**
        *   Start by clearly stating if the rule applies to the integration and why.
        *   If it applies, explain in detail whether the integration follows the rule or not.
        *   **Crucially, reference specific parts of the provided code or identify missing components/patterns to justify your assessment.**
        *   If the rule does not apply (status "exempt"), expand on the "Reason" from the table, providing more context if necessary.

4.  **Suggestions Section (Conditional):**
    *   **Heading:** `## Suggestions`
    *   **Content:**
        *   **Only include this section if the Status is "todo".**
        *   Provide clear, actionable, and specific steps the developer can take to make the integration compliant with the rule.
        *   If possible, include examples of code changes or additions.
        *   Explain *why* these changes would satisfy the rule.
        *   If Status is "done" or "exempt", omit this entire section or state "No suggestions needed."

**Analysis Process for You (the AI):**

1.  **Understand the Rule:** Carefully read and interpret rule content to understand its purpose, requirements, and scope.
2.  **Applicability Check:** Based on rule content and your knowledge of Home Assistant integrations, determine if this specific rule is relevant and applicable to the provided `{integration}` code.
3.  **Code Review (if applicable):** If the rule applies, thoroughly examine the provided integration code.
    *   Look for specific patterns, functions, configurations, or architectural choices mentioned or implied by the rule.
    *   Identify if the integration implements these requirements correctly.
    *   Note any deviations, omissions, or incorrect implementations.
4.  **Determine Status:** Based on your analysis, assign "todo", "done", or "exempt".
5.  **Formulate Report:** Construct the report according to the specified structure, ensuring your reasoning is clear, evidence-based (referencing code), and constructive.

--- START OF ATTACHED FILES ---

--- FILE: rule-{rule}-description.md ---
{rule_content}
--- END FILE ---

{files}

--- END OF ATTACHED FILES ---
""".strip()

IGNORED_RULES = (
    # Has to verify docs repo
    "docs-",
    # Has to verify brands repo
    "brands",
    # Has to verify PyPI
    "dependency-transparency",
    # Has to verify tests
    "config-flow-test-coverage",
    "test-coverage",
)


def get_quality_scale_rules(core_path: Path) -> dict[str, list[str]]:
    """
    Get quality scale rules from the core repository.
    """
    rules_file = core_path / "script" / "hassfest" / "quality_scale.py"
    module = ast.parse(rules_file.read_text(encoding="utf-8"))
    rules = {
        "BRONZE": [],
        "SILVER": [],
        "GOLD": [],
        "PLATINUM": [],
    }
    for node in module.body:
        if not isinstance(node, ast.Assign) or node.targets[0].id != "ALL_RULES":
            continue

        for rule in node.value.elts:
            if not isinstance(rule, ast.Call) or rule.func.id != "Rule":
                continue

            rule_name = rule.args[0].value
            rule_tier = rule.args[1].attr
            rules[rule_tier].append(rule_name)

        break
    return rules


def get_integration_files_for_prompt(integration_path: Path) -> str:
    """
    Get all files for the integration to be used in the prompt.
    """
    name = integration_path.name
    integration_files = []
    priority_extensions = {
        "manifest.json": 1,
        "__init__.py": 2,
        ".py": 5,
        ".yaml": 8,
        ".json": 10,
    }
    for file_path in sorted(
        integration_path.rglob("*"),
        key=lambda x: (
            # Try by name first
            priority_extensions.get(
                x.name,
                # Then by extension
                priority_extensions.get(x.suffix, 0),
            ),
        ),
    ):
        if "__pycache__" in file_path.parts or not file_path.is_file():
            continue

        integration_files.append(
            f"\n\n--- FILE: homeassistant/components/{name}/{file_path.relative_to(integration_path)} ---\n\n"
        )
        integration_files.append(file_path.read_text(encoding="utf-8"))
        integration_files.append(f"\n--- END FILE ---")

    return "".join(integration_files).strip()


def estimate_tokens(prompt: str, model: str = "gpt-4") -> int:
    encoding = tiktoken.encoding_for_model(model)
    tokens = encoding.encode(prompt)
    return len(tokens)


def get_args() -> tuple:
    """
    Get command line arguments.
    """
    import argparse

    parser = argparse.ArgumentParser(
        description="Check quality scale for an integration."
    )
    parser.add_argument(
        "integration",
        help="The integration to check.",
        type=str,
    )
    parser.add_argument(
        "--core-path",
        help="Path to Home Assistant core.",
        type=str,
        default="../core",
    )
    parser.add_argument(
        "--target-scale",
        help="Quality scale to target.",
        choices=["bronze", "silver", "gold", "platinum"],
    )
    parser.add_argument(
        "--force-update",
        help="Will also update existing reports.",
        action="store_true",
    )
    parser.add_argument(
        "--dry-run",
        help="Do not generate reports.",
        action="store_true",
    )
    parser.add_argument(
        "--free-model",
        help="Use less powered but free model.",
        action="store_true",
    )
    parser.add_argument(
        "--include-done",
        help="Generate reports for rules marked done or exempt.",
        action="store_true",
    )
    parser.add_argument(
        "--single-rule",
        help="Only run the first applicable rule.",
        action="store_true",
    )
    return parser.parse_args()


def main(token: str, args) -> None:
    """
    Main function to run the script.
    """
    core_path = Path(args.core_path).resolve()

    if not core_path.is_dir():
        logger.error(f"Core path {core_path} does not exist.")
        sys.exit(1)

    integration_path = core_path / "homeassistant" / "components" / args.integration

    if not integration_path.is_dir():
        logger.error(f"Integration path {integration_path} does not exist.")
        sys.exit(1)

    rules = get_quality_scale_rules(core_path)

    manifest = json.loads(
        (integration_path / "manifest.json").read_text(encoding="utf-8")
    )
    integration_quality_scale = manifest.get("quality_scale", "unknown").upper()
    if (
        integration_quality_scale not in ("LEGACY", "UNKNOWN")
        and integration_quality_scale not in rules
    ):
        logger.error(
            f"Integration quality scale {integration_quality_scale} is not supported."
        )
        sys.exit(1)

    rules_report = {
        rule: {"status": "todo"} for scale in rules for rule in rules[scale]
    }
    rules_report_path = integration_path / "quality_scale.yaml"
    if rules_report_path.exists():
        with open(rules_report_path, "r", encoding="utf-8") as file:
            rules_report.update(
                {
                    rule: {"status": info} if isinstance(info, str) else info
                    for rule, info in yaml.safe_load(file)["rules"].items()
                }
            )

    output_dir = OUTPUT_DIR / args.integration
    rules_to_check = {}
    printed_something = False
    for quality_scale, rules in rules.items():
        for rule in rules:
            if rule.startswith(IGNORED_RULES):
                continue

            info = rules_report[rule]

            if info["status"] != "todo" and not args.include_done:
                continue

            report_path = output_dir / f"{rule}.md"

            if report_path.exists() and not args.force_update:
                print(f"Report for {rule} already exists. Skipping.")
                printed_something = True
                continue

            rules_to_check[rule] = report_path

        # Stop if we reached the target scale or found some rules to check
        # in the current scale
        if args.target_scale:
            if quality_scale == args.target_scale.upper():
                break
        elif rules_to_check:
            break

    if printed_something:
        print()
    print("Generating report for rules:")
    for idx, rule in enumerate(rules_to_check):
        print(f"  {rule}")
        if idx == 0 and args.single_rule:
            print()
            print("Ignoring next rules due to --single-rule flag:")
    print()

    integration_files = get_integration_files_for_prompt(integration_path)

    if args.dry_run:
        prompt = RULE_REVIEW_PROMPT.format(
            integration=args.integration,
            rule=rule,
            rule_url=QUALITY_SCALE_RULE_DOCS_URL.format(rule),
            rule_content=requests.get(QUALITY_SCALE_RULE_RAW_URL.format(rule)).text,
            files=integration_files,
        )
        print("Dry run enabled. Not generating reports.")
        print(f"Prompt token estimate: {estimate_tokens(prompt)}")
        return

    client = genai.Client(api_key=token)
    output_dir.mkdir(parents=True, exist_ok=True)

    model = FREE_MODEL if args.free_model else PAID_MODEL

    for rule, report_path in rules_to_check.items():
        start_time = datetime.datetime.now()
        response = client.models.generate_content(
            model=model,
            contents=RULE_REVIEW_PROMPT.format(
                integration=args.integration,
                rule=rule,
                rule_url=QUALITY_SCALE_RULE_DOCS_URL.format(rule),
                rule_content=requests.get(QUALITY_SCALE_RULE_RAW_URL.format(rule)).text,
                files=integration_files,
            ),
        )

        report = response.text
        report += """

_Created at {datetime}. Prompt tokens: {prompt_tokens}, Output tokens: {output_tokens}, Total tokens: {total_tokens}_
""".format(
            datetime=datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            prompt_tokens=response.usage_metadata.prompt_token_count,
            output_tokens=response.usage_metadata.candidates_token_count,
            total_tokens=response.usage_metadata.total_token_count,
        )

        end_time = datetime.datetime.now()
        duration = end_time - start_time

        report_path.write_text(report, encoding="utf-8")
        print(f"Report for {rule} generated at {report_path.relative_to(SCRIPT_DIR)} (took {duration.total_seconds():.1f}s)")
        print()
        if args.single_rule:
            break


if __name__ == "__main__":
    token_path = Path(".token")
    if not token_path.exists():
        print("No token file found. Please create a .token file with your API key.")
        sys.exit(1)
    token = token_path.read_text(encoding="utf-8").strip()
    main(token, get_args())
